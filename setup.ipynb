{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee62f01",
   "metadata": {},
   "source": [
    "# Welcome to the Correlation Example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda79260",
   "metadata": {},
   "source": [
    "## Workshop Steps\n",
    "Now that you have opened up the MyBinder environment and are reading this, you are already on the right track! Inside this environment,\n",
    "you will also find:\n",
    "* **sample scripts**: This folder contains the base of the scripts that you will be working with to complete the exercise. Please look for the triple exclamation points (!!!) as that means that you are being asked to write some code to get things to work!\n",
    "* **README.md**: This is the README file you saw on the Github page containing general information on the scenario.\n",
    "* **requirements.txt**: A list of the required Python packages (+ their respective versions) that were installed upon startup of MyBinder.\n",
    "* **start** and **postBuild**: System files containing system configuration details for the MyBinder environment.\n",
    "* **setup.ipynb**: The file you are reading right now! Think of this as your home page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af82213d",
   "metadata": {},
   "source": [
    "### Step 1: Create a Data Job\n",
    "Based on what we already learnt in the previous scenarios, let's create a new data job. Since this example uses the VDK Control Service, the data job will be automatically registered on the cloud and needs to have a unique name. \n",
    "\n",
    "Remember to store the data job in a sub-folder of the home directory (\"/home/jovyan\") because the Streamlit script (*build_streamlit_dashboard.py*) needs to be outside of the data job folder. \n",
    "\n",
    "***Reminder:** You can always run `!vdk --help` to remind yourself of the vdk commands.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31519980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please finish the vdk command for creation of data job\n",
    "!vdk ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bfd927",
   "metadata": {},
   "source": [
    "### Step 2: Work Out the Data Job Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf90990",
   "metadata": {},
   "source": [
    "Now that you have created a data job, please go inside the subfolder and set up the structure of your data job. As in the rest of the examples, we have preliminary prepared the scripts in the \"sample scripts\" folder, but there are some coding challenges in them for you to play around!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c7205-039f-4eac-aaa1-add4957f8570",
   "metadata": {},
   "source": [
    "Let's first delete the template files from the data job folder that we will not need by running the below command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939e55f6-9df1-4905-937b-741e3c0d819d",
   "metadata": {},
   "source": [
    "<font color='red'>**TO DO!**</font>\n",
    "Remember to replace \"data-job-name\" with the name of your own data job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417e94a2-25e0-4d5c-9230-1d04fea163ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm \"data-job-name/10_sql_step.sql\"\n",
    "! rm \"data-job-name/20_python_step.py\"\n",
    "! rm \"data-job-name/README.md\"\n",
    "! rm \"data-job-name/requirements.txt\"\n",
    "! rm \"data-job-name/config.ini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337470de-037d-419f-9475-634170484a55",
   "metadata": {},
   "source": [
    "Now let's move the sample scripts to the data job subfolder. Please run the code below.\n",
    "\n",
    "<font color='red'>**TO DO!**</font>\n",
    "Remember to replace \"data-job-name\" with the name of your own data job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb46a76-10a4-40c7-9d31-f38b7a756f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv \"sample scripts/01_create_covid_cases_usa_daily.sql\" ~/data-job-name\n",
    "! mv \"sample scripts/02_create_yankee_candle_reviews.sql\" ~/data-job-name\n",
    "! mv \"sample scripts/03_create_yankee_candle_reviews_transformed.sql\" ~/data-job-name\n",
    "! mv \"sample scripts/04_create_weekly_correlation.sql\"  ~/data-job-name\n",
    "! mv \"sample scripts/10_ingest_covid_data.py\" ~/data-job-name\n",
    "! mv \"sample scripts/20_ingest_amazon_reviews.py\" ~/data-job-name\n",
    "! mv \"sample scripts/30_transform_amazon_reviews.py\" ~/data-job-name\n",
    "! mv \"sample scripts/40_calculate_correlation.py\"  ~/data-job-name\n",
    "! mv \"sample scripts/webscrape.py\" ~/data-job-name\n",
    "! mv \"sample scripts/config.ini\" ~/data-job-name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e8c66a",
   "metadata": {},
   "source": [
    "**Great!** Now you're all set up with the data job:\n",
    "* You have created a data job.\n",
    "* You deleted the template files that you do not need.\n",
    "* You moved the sample scripts we provided to the data job sub-folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542bdd7",
   "metadata": {},
   "source": [
    "The next step is to begin working on each script in the data job. Let's get it going!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea47ca4c",
   "metadata": {},
   "source": [
    "### Step 3: Data Job - Create the necessary tables in the DB (scripts 01-04.sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2454c77-45fe-4325-8101-649c069ea4d8",
   "metadata": {},
   "source": [
    "The first four scripts (.sql) take care of creating the tables that will store the raw and transformed data in the cloud Trino DB. \n",
    "The SQL engine would check whether tables with such names already exist in the DB and if not, they will be created, otherwise the step will be skipped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b303b0a-cb95-46f9-ab58-5a43d3bec0be",
   "metadata": {},
   "source": [
    "<font color='red'>**TO DO!**</font> Please open each of the files and make the necessary insertions:\n",
    "* 01_create_covid_cases_usa_daily.sql - insert a unique table name in line 3\n",
    "* 02_create_yankee_candle_reviews.sql - insert a unique table name in line 3\n",
    "* 03_create_yankee_candle_reviews_transformed.sql - insert a unique table name in line 4\n",
    "* 04_create_weekly_correlation.sql - insert a unique table name in line 8 and the column definitions in line 9 onwards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa93b7c5",
   "metadata": {},
   "source": [
    "<font color='green'>**GOOD JOB!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc45ff2f",
   "metadata": {},
   "source": [
    "Congratulations! You've finished with the first part of the data job! \n",
    "\n",
    "**Please remember to save the scripts before continuing further.** \n",
    "\n",
    "At this point, you could try running the job in order to:\n",
    "* Check if the first 4 scripts run sucessfully.\n",
    "* See whether the tables are actually created in the DB.\n",
    "* Examine the error messages, as the other scripts will throw them as of now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86591bc1-b1d0-4caa-8da6-4e97a450bccb",
   "metadata": {},
   "source": [
    "<font color='red'>**TO DO!**</font>\n",
    "Remember to replace \"data-job-name\" with the name of your own data job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60696e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! vdk run data-job-name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a76230-b7be-4f06-b2ec-82027c7e8d35",
   "metadata": {},
   "source": [
    "You can now check whether the tables have been created successfully by running the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e40511b-9370-47f0-8d5d-ca4de15f1fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SQL extention\n",
    "%config SqlMagic.autocommit=False \n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c038dd-3182-418b-8c7c-a4d4cbdc2fd2",
   "metadata": {},
   "source": [
    "<font color='red'>**TO DO!**</font>\n",
    "Remember to replace \"name-of-the-table\" with the name of the respective table you want to query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644ea40-1a28-4723-b996-5526b8f66b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select * from mysql.default.name-of-the-table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a673786e",
   "metadata": {},
   "source": [
    "### Step 4: Data Job - Ingest the daily COVID-19 data (10_ingest_covid_data.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d295b55",
   "metadata": {},
   "source": [
    "The aim of this script is to **ingest the cumulative daily COVID-19 data for the US using an [API](https://github.com/M-Media-Group/Covid-19-API)**. This should already be familiar to you from the second example that you already built! The difference here is that we will be extracting US data rather than data for European countries, and we will only need the daily COVID cases. Again, we will use the **incremental ingestion approach**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534a81a",
   "metadata": {},
   "source": [
    "As usual, we start with importing libraries and initializing the `run` method of the `job_input` object. We then retrieve any saved data job properties if the job has been run before, or initialize the \"last_date_covid\" property if there is no such property from before. It will store the last ingested date for this particular table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89ea1ac",
   "metadata": {},
   "source": [
    "The next step is to **make a GET request** to the COVID API and extract from the returned dictionary only the necessary data - date and number of cases. This information is saved in a dictionary and then transformed into a pandas dataframe. Based on the value of the \"last_date_covid\" property, we keep only the records which have not been ingested into the table already. If there are any records left in the dataframe after the applied filtering, these are ingested into the respective table (the table created in script \"01_create_covid_cases_usa_daily.sql\") using the `send_tabular_data_for_ingestion` method of VDK's `job input` object. The last step is to reset the \"last_date_covid\" property value to the latest date in the COVID source DB table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c6a3b2",
   "metadata": {},
   "source": [
    "<font color='red'>**TO DO!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43f6619",
   "metadata": {},
   "source": [
    "We have a couple of tasks for you to complete in this script:\n",
    "* **line 21** - add VDK's job_input method that is used to get all job properties. The documentation on all VDK's job_input methods available can be found [here](https://github.com/vmware/versatile-data-kit/blob/246008c8fffcac173b6ac3f434814acb6faf16a7/projects/vdk-core/src/vdk/api/job_input.py#L11).\n",
    "* **line 31** - add the appropriate method of the requests package that makes a GET API request using the URL defined on line 28.\n",
    "* **line 53** - add name of the table created in script \"01_create_covid_cases_usa_daily.sql\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b19c2e7",
   "metadata": {},
   "source": [
    "<font color='green'>**GOOD JOB!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189fc832",
   "metadata": {},
   "source": [
    "Congratulations! You finished the second part of the data job as well! To check the completed script works as expected, you can run the data job again and look at the status of the various scripts. You could track the log messagesand see how many records were ingested in the first DB table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d1901a-66d2-4323-b254-68a24a1f2e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to replace \"data-job-name\" with the name of your own data job\n",
    "! vdk run data-job-name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439806b6-beb8-4837-b69d-2c5f2d902329",
   "metadata": {},
   "source": [
    "Let's also check the contents of the table in which we just ingested the COVID-19 records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba375db3-50dc-421e-91fc-ea369276079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to replace \"name-of-the-table\" with the name of the respective table you want to query.\n",
    "%sql select * from mysql.default.name-of-the-table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce59fa6-a10e-45d0-ad77-d40fae85a60a",
   "metadata": {},
   "source": [
    "<font color='orange'>**ADVICE:**</font>\n",
    "In case there were issues with running the script that you just filled out and you want to go back and retry, there are a few actions you need to take before rerunning the data job:\n",
    "* If there were records ingested in the table that stores the daily COVID-19 data, you need to delete it. To do this, execute the following statement: `%sql drop table mysql.default.name-of-the-table` (remember to replace \"name-of-the-table\" with the name of the respective table you want to delete). Once you rerun the data job, the table will be recreated as per script \"01_create_covid_cases_usa_daily.sql\"\n",
    "* You also need to reset the \"last_date_covid\" job property. This will ensure that once you rerun the data job and the table has been recreated empty,  all records since 2020-01-01 will be reingested again. To do this, in script \"10_ingest_covid_data.py\" replace lines 21-25 with the following:\n",
    "\n",
    "```python \n",
    "props = job_input.get_all_properties()\n",
    "props['last_date_covid'] = '2020-01-01'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f9b273",
   "metadata": {},
   "source": [
    "### Step 5: Data Job - Ingest Amazon reviews (20_ingest_amazon_reviews.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51878585",
   "metadata": {},
   "source": [
    "With the next script things become even more interesting! Here we will scrape the text of all negative Amazon reviews for one of the [most popular Yankee candles](https://www.amazon.com/Yankee-Candle-Large-Balsam-Cedar/dp/B000JDGC78/ref=cm_cr_arp_d_product_top?ie=UTF8) and will ingest them into a table in our Trino DB. In simple terms, webscaping is basically extracting data from web pages. In Python, one of the most popular packages which provides web scraping functionalities is the [BeautifulSoup package](https://pypi.org/project/beautifulsoup4/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003f9b3d",
   "metadata": {},
   "source": [
    "**Let's open the script and walk through what it does.**\n",
    "\n",
    "We start by performing some already familiar actions - importing packages, initializing the logger, opening up job_input's `run` function and initializing the data job property that we will use to incrementally ingest the Amazon reviews in the respective DB table (\"last_date_amazon\" property).\n",
    "\n",
    "<font color='red'>**TO DO!**</font> Use the hints from script \"10_ingest_covid_data.py\" and initialize the \"last_date_amazon\" property yourself! (line 28)\n",
    "\n",
    "<font color='green'>**GOOD JOB!**</font>\n",
    "\n",
    "Next we initialize a few variables and then enter into a **while loop** (line 37 in the script). The idea of this while loop is to go through the pages with critical Amazon reviews (each page contains 10 reviews, you can check this out on the Amazon website linked above) until the review date reaches back to the last date that is already present in our DB table (as indicated by the \"last_date_amazon\" job property). In the first iteration of the data job, we will ingest all reviews from 2020-01-01 until today. To iterate over the pages, we use a **parameter** that is inserted into the URL of the web page and is being increased by 1 at the end of each iteration of the while loop (line 77 in the script).\n",
    "\n",
    "In lines 44-48 of the script we perform **the actual webscraping**. As you can see, we use 3 methods of the webscrape object - `html_code`, `cus_rev` and `rev_date`. Those methods are defined in the file \"webscrape.py\" that if you remember we moved from the \"sample scripts\" folder to the data job folder in the beginning of this walkthrough. \n",
    "\n",
    "<font color='red'>**TO DO!**</font> Open the \"webscrape.py\" script and familiarize yourself with the function definitions. How did we manage to link the two scripts and use the functions defined in \"webscrape.py\" into the \"20_ingest_amazon_reviews.py\" script? *Hint: look at line 8 in \"20_ingest_amazon_reviews.py\".*\n",
    "\n",
    "<font color='green'>**GOOD JOB!**</font>\n",
    "\n",
    "Since the webscrape object's methods return data only for the current Amazon review page, we need to append the already scraped results into another object that will **store cumulatively all scrapred results** - we've chosen a Python list object to do the job. In lines 50-70, we append the review dates and the actual text reviews into 2 list objects - `rev_result` and `date_result` using for loops. In those for loops, we also perform some cleaning:\n",
    "* remove reviews with no text - only picture, video or score (lines 52-53 and 60-61).\n",
    "* transform the review date from the format \"Reviewed in the United States on February 14, 2022\" to a datetime Python object that looks like \"2022-02-14\" (lines 63-69).\n",
    "\n",
    "In lines 73-74, we have another while loop which makes sure that for every review page we scrape, the number of scraped reviews matches the number of review dates or in other words, the length of both lists (`rev_result` and `date_result`) are the same. This step is necessary because in lines 52-53 we skip the reviews containing only images or videos (i.e. review text = \"The media could not be loaded.\"). This means that at each page, we might end up having more review dates than actual review texts. The `date_result.pop(-1)` (line 74) basically removes the last item of the date_result list until the length of `rev_result` and `date_result` are the same. The following example illustrates what the code does in practice:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a9106-3fbd-4f12-9103-2e8b8dd1a3ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Step 1:**\n",
    "\n",
    "| date_page | rev_page | \n",
    "| --- | --- |\n",
    "| 3/10/2022 | This candle has no scent. |\n",
    "| 3/11/2022 | The media could not be loaded. |\n",
    "| 3/12/2022 | The smell used to be much stronger before. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f583eb-1813-4f4b-b164-193b08abfc4a",
   "metadata": {},
   "source": [
    "**Step 2:**\n",
    "\n",
    "| date_page | rev_page | \n",
    "| --- | --- |\n",
    "| 3/10/2022 | This candle has no scent. |\n",
    "| 3/11/2022 | The smell used to be much stronger before. |\n",
    "| 3/12/2022 | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ee7a3f-20ab-4893-bdb4-68be38af6af7",
   "metadata": {},
   "source": [
    "**Step 3:**\n",
    "\n",
    "| date_result | rev_result | \n",
    "| --- | --- |\n",
    "| 3/10/2022 | This candle has no scent. |\n",
    "| 3/11/2022 | The smell used to be much stronger before. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea22f9e9-6cb2-4bc8-af0e-cd4a87220681",
   "metadata": {},
   "source": [
    "Of course, this is not the best method that can be chosen since on some pages we might end up with a few days discrepancy between the actual date a given review is written and the date we assign it to in the DB, but since those cases are rare (3-4 such reviews for the entire period since January 2020) and we aim to not overcomplicate the solution, this is a good approximation of the results. \n",
    "\n",
    "<font color='red'>**TO DO!**</font> In your spare time, think of a more advanced solution to perfectly match the review dates with the actual text reviews on each page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60edd85-edbd-4d63-ba76-94bb101d2a44",
   "metadata": {},
   "source": [
    "After we finish going through the review pages and go out of the while loop, we **zip the two lists into 1 object and convert it to a pandas data frame** (line 80). Since the while loop that goes through the review pages will always execute at least once (current timestamp > last ingested review), the first review page will always be scraped. However, it might contain reviews that were already ingested into the DB. To prevent this, we use a filter that keeps only the non-ingested records using the \"last_date_amazon\" job property (line 83). \n",
    "\n",
    "In lines 84-87 we also perform some **data cleaning** which removes emoji characters from the text reviews. This step is needed since emojis have a non-standard encoding which breaks ingestion into our Trino DB. As you can see, here we also use a method defined in the \"webscrape.py\" script - `remove_emoji`.\n",
    "\n",
    "<font color='red'>**TO DO!**</font> Go to the \"webscrape.py\" script and investigate what the `remove_emoji` method does. Which external package does it use? Google it and find out more about regular expressions and what they do!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5912024e-e6dc-4db0-8ac3-9a0a26f52f48",
   "metadata": {},
   "source": [
    "We're almost at the end of this script! What is left for us to do is to **ingest** the dataframe values into the DB table we've created in script \"02_create_yankee_candle_reviews.sql\" and **reset the \"last_date_amazon\" job property** (lines 90-99).\n",
    "\n",
    "<font color='red'>**TO DO!**</font> Use your knowledge and hints from the previous script and enter the arguments of the `send_tabular_data_for_ingestion` method by yourself!\n",
    "\n",
    "One last thing to pay attention to - in line 103 we \"pause\" the data job execution for 10 seconds. This step is necessary because there is some latency between the execution of the script and the actual ingestion of the data into the DB table. Since the next script that we will be working on (30_transform_amazon_reviews.py) reads from the table that we are now ingesting into, we have to make sure that all needed records will be present in table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983fa46",
   "metadata": {},
   "source": [
    "<font color='green'>**GOOD JOB!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1c40c1",
   "metadata": {},
   "source": [
    "**That was a challenge!**\n",
    "\n",
    "Let's run the job again and then query the Amazon reviews table just to make sure that the insertions we made in the script are correct. **In case you observe errors in the script that you just editted**, please make sure to repeat the steps we explained in Step 4 of this walkthrough on deleting the Amazon reviews table and resetting the \"last_date_amazon\" job property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd322b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to replace \"data-job-name\" with the name of your own data job\n",
    "! vdk run data-job-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04723761-74cf-43d7-8f2a-8382c9242064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to replace \"name-of-the-table\" with the name of the respective table you want to query.\n",
    "%sql select * from mysql.default.name-of-the-table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c0c29",
   "metadata": {},
   "source": [
    "### Step 6: Data Job - Transform the raw Amazon reviews data (30_transform_amazon_reviews.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72b5fe1",
   "metadata": {},
   "source": [
    "The next milestone in our data job is to read the raw Amazon data we've just ingested and do some transformations. We need to first **flag all reviews containing the words \"scent\", \"smell\" or \"fragnance\"**. This will indicate which of the critical reviews are (potentially) complaints about the candles having no scent. Next, we need to **count how many \"no scent\" reviews there are per day**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a53c81-cfd7-4a0c-a751-e077d60a8b1a",
   "metadata": {},
   "source": [
    "As in the rest of the scripts, we will be saving data into a new table so we will need a job property to store the last ingested date.\n",
    "\n",
    "<font color='red'>**TO DO!**</font> \n",
    "\n",
    "Your first task is to **complete the job property definition**, this time entirely by yourself! You need to first get all job properties, then check whether a property with the name \"last_date_amazon_transformed\" already exists in the properties dictionary and if not, initialize it with a value of '2020-01-01'. This definition will be analogous to the the job properties definitions we made in the previous 2 scripts so you can use them as hints. Provide you inputs starting from line 26.\n",
    "\n",
    "<font color='green'>**GOOD JOB!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa05e3d",
   "metadata": {},
   "source": [
    "In the next part of the code, we use `job_input`'s `execute_query` method to fetch all records from the table with raw Amazon reviews that we've already populated in the previous script (lines 29-36). Please note that we provide a parameter to the SQL query which is the value of \"last_date_amazon_transformed\" job property. In this way once the job starts running on a regular basis, we will be selecting only the newly populated reviews that have not been \"transformed\" yet. **Remember to put the name of the table you populated in script 20_ingest_amazon_reviews.py in line 32.**\n",
    "\n",
    "<font color='orange'>**NB:**</font> Have you noticed that the `execute_query` method automatically \"knew\" in which database to look for our table? How cool is that! It's actually pretty easy to setup VDK to work in accordance with any database, you need to just add the configuration settings in the \"config.ini\" file of the data job. We will investigate this file in more details once we reach the step on data job deployment.\n",
    "\n",
    "<font color='red'>**TO DO!**</font> Your next task is to transform the result from the query (`reviews_raw`) into a pandas data frame (line 37). To achieve that, you need to [initialize the DataFrame class of the pandas package](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). \n",
    "*Hint: On line 80 in script \"20_ingest_amazon_reviews.py\" we make a similar definition that you can use as reference, except that instead of the zipped lists, you need to use the `reviews_raw` object.*\n",
    "\n",
    "<font color='green'>**GOOD JOB!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83dfd05",
   "metadata": {},
   "source": [
    "Next, we **make the necessary transformations/aggregations** if there is any data returned from the query:\n",
    "\n",
    "We create a new column in the data frame called 'flag_no_scent' with value TRUE for the cases when the review text contains the words \"scent\", \"fragrance\" or \"smell\" (lines 41-43). Then we use pandas groupby functionality to count the total number of negative reviews (lines 45-47)  and the number of \"no scent\" reviews per day (49-51). After that we merge the two \"sub-dataframes\" with the newly calculated columns and fill all missing values with 0 (line 55). The resulting dataframe has 3 columns - date, num_negative_reviews and num_no_scent_reviews, and is ready to be ingested into the table that we created in script \"03_create_yankee_candle_reviews_transformed.sql\" (**remember to put its name in line 61**).\n",
    "\n",
    "<font color='orange'>**HOMEWORK:**</font> As you probably already suspect, the way in which we flag the \"no scent\" reviews has its flaws since there might be negative reviews which criticize something about the candle, but say that it smells good. These cases are rare, but do appear in the data, so our approach to flagging those reviews would produce some \"false positive\" results. For the purposes of the example and for simplicity, we decided to leave those as they are, but in your spare time you could make a research on more advanced text analytics approaches and try to improve this example. Feel free to experiment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fcc58f-d149-4062-b3a5-b2037aadded7",
   "metadata": {},
   "source": [
    "<font color='red'>**TO DO!**</font>\n",
    "\n",
    "Your last task in this script would be to update the value of the \"last_date_amazon_transformed\" job property by taking the maximum value of the 'date' column in the df_group data frame (line 64).\n",
    "\n",
    "<font color='green'>**GOOD JOB!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed214bf1",
   "metadata": {},
   "source": [
    "What is left for us before continuing to the last data job script is to run the data job again and observe whether this script will execute successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3eadbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to replace \"data-job-name\" with the name of your own data job\n",
    "! vdk run data-job-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb2ea0-66ff-42e4-92cc-b61427f8cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to replace \"name-of-the-table\" with the name of the respective table you want to query.\n",
    "%sql select * from mysql.default.name-of-the-table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13adb3e-12ce-417b-a02c-76c113b4833e",
   "metadata": {},
   "source": [
    "In case of errors in the edits that you just made, remember to delete the table you just ingested into and reset the \"last_date_amazon_transformed\" job property before rerunning the data job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144da9dd-d1d5-46d2-984d-f20721c67e4c",
   "metadata": {},
   "source": [
    "### Step 7: Data Job - Make weekly aggregations and calculate correlation coefficients (40_calculate_correlation.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf2735-9a80-4466-8869-5e1bb758e96a",
   "metadata": {},
   "source": [
    "In the last script, we **transform the daily COVID and reviews data to weekly** (as of the Monday of each week) and **recalculate the correlation coefficients** for each new week - i.e. as new weekly data comes in, the time series for COVID cases and number of \"no scent\" reviews are enriched which means that the correlation coefficient as of the given week will change accordingly.\n",
    "\n",
    "We start in the usual way - by defining a new job property for the correlation table (lines 27-31). We then read the transformed Amazon reviews data and the daily COVID-19 data and transform the results into dataframes (lines 33-49). \n",
    "\n",
    "<font color='red'>**TO DO!**</font>\n",
    "\n",
    "In lines 34 and 43, add the VDK's method that enables executing SQL statements from Python scripts inside data jobs. *Hint: we used it in the previous script as well.* Also, remember to put the correct names of the tables in lines 37 and 46.\n",
    "\n",
    "<font color='green'>**GOOD JOB!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6585e4-8930-464c-bd68-2e4b51cc7055",
   "metadata": {},
   "source": [
    "In line 52, we merge the two data frames from the previous step and sort by date descending. Then we start with the **transformations**. First, since the number Ð¾f COVID-19 cases in the source table are cumulative numbers, we find the new COVID cases diagnosed for the day in lines 57-58. The aggregation of data to weekly is done in lines 60-68 using pandas `resample` method that can make periodic aggregations by performing different calculations - in our case, summing the values. \n",
    "\n",
    "Lines 70-76 take care of **calculating the correlation coefficients** and adding them as a column in the data frame. This is done through a for loop that limits the number of observations in the time series and in this way calculates a correlation coefficients as of each week. For example, in the last week of January 2020, the correlation coefficient will be caculated taking into account only the data prior to this date, while the correlation coefficient for last week will take into account all data recorded in the table so far. In this way we would be able to track how the correlation coefficients change over time. \n",
    "\n",
    "<font color='red'>**TO DO!**</font>\n",
    "\n",
    "Line 79 requires your input! Since the 'date' column in the `df_merged_weekly` data frame is in datetime format (i.e. it looks like \"2022-02-06T00:00:00\"), we want to transform it to look like \"2022-02-06\". There is a pandas datetime method which handles such conversions - Google it and find out which! *Hint: we used this method in script \"20_ingest_amazon_reviews.py\".*\n",
    "\n",
    "<font color='green'>**GOOD JOB!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e1badf-dcfc-48c5-a718-6c9c79c952aa",
   "metadata": {},
   "source": [
    "We then keep only the records that have not been ingested in the table so far based on the \"last_date_correlation\" job property (line 82).\n",
    "\n",
    "In lines 85-95 we ingest the dataframe values into the weekly correlation table and reset the value of the \"last_date_correlation\" property in case there are any records left after the filtering. **Remember to put the name of the table you created in script 04_create_weekly_correlation.sql in line 90.**\n",
    "\n",
    "We finished with all the necessary edits! Let's run the job for the last time and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a608a21b-7897-455f-a352-e53f9523f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to replace \"data-job-name\" with the name of your own data job\n",
    "! vdk run data-job-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08978195-e7dc-4e9a-990e-4a3bc2ebd71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to replace \"name-of-the-table\" with the name of the respective table you want to query.\n",
    "%sql select * from mysql.default.name-of-the-table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3134393",
   "metadata": {},
   "source": [
    "You should now get a success message for every single script, as well as one for the entire data job above them. **If so, congratulations! You have built the entire data job!**\n",
    "\n",
    "You can actually observe the incremental ingestion effect even now! As we have printed meaningful trace messages into the log, you should see statements like \"Success! 0 rows were inserted in ... table.\" and \"No new records to ingest.\" for the first 3 ingestion scripts. The only ingestion should now happen in the table containing the weekly correlation coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4a5b54-c893-44da-a0ed-92a345562d5e",
   "metadata": {},
   "source": [
    "### Step 8: Schedule the Data Job for Regular Execution and Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300bd949-d0cf-466f-8027-0437cd41e8bc",
   "metadata": {},
   "source": [
    "Since the correlation analysis that we perform is on a weekly basis, it makes sense to schedule our data job to run once per week. VDK allows the **automatic execution of data jobs by deploying them on a cloud server** which handles the regular execution as per schedule that the user defines. The deployment configurations are entered in the **\"config.ini\"** file that is required for deployment. If you remember, we already moved a file with such a name from the \"sample scripts\" folder to the folder of our data job.\n",
    "\n",
    "Let's open it up and examine the contents.\n",
    "\n",
    "In the first section [owner], we have specified the **team owning the data job**. In the second section [job] we defined the schedule of execution. It is in cron format (you can use [this website](https://crontab.guru/#*/20_*_*_*_*) to translate the cron schedule into a human-readable form). In this case, we want the schedule to run on the Monday of each week at 00:01am US time. Since VDK uses UTC time for schedule execution, the cron schedule indicates 05:01am UTC time. \n",
    "\n",
    "The config file could also include a [contacts] section which specifies whether any **notifications** are sent to specific emails upon job execution success, failure or deployment. In our case, we have left those empty.\n",
    "\n",
    "The last part of the config file contains the **VDK configuration settings** - the type of DB to which we will be ingesting, the DB location, schema and catalogue. \n",
    "\n",
    "For a full list and explanations of the configuration settings you could enter into the \"config.ini\" file of a data job, you can run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3c6a7e-fc8d-44f1-b4a1-677f8bb4d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!vdk config-help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a3ab9a-0aa1-4ff3-82fb-9259f510f28f",
   "metadata": {},
   "source": [
    "<font color='red'>**TO DO!**</font> \n",
    "\n",
    "Let's now deploy the data job!\n",
    "\n",
    "Run the command below, but first **remember to replace name-of-data-job with your data job name** after the \"-n\" and in the directory pathway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c148957-65ea-4d99-8039-86c15afa65f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!vdk deploy -n name-of-data-job -r \"Initial deploy\" -p /home/jovyan/name-of-data-job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0830f150-cdc6-4afb-bbd8-bfad9b077e85",
   "metadata": {},
   "source": [
    "After this is done, follow the prompts in the displayed log to check the deployment status of your data job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4901e1-aa56-46f2-ba15-32ce5ba26473",
   "metadata": {},
   "source": [
    "<font color='green'>**GOOD JOB!**</font>\n",
    "\n",
    "Our DB will now be enriched with new data every week! We are ready to proceed with the dashboard visualizations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd7bab9",
   "metadata": {},
   "source": [
    "### Step 9: Build and Run a Streamlit Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eebd8d",
   "metadata": {},
   "source": [
    "Now that we have finished with the data job, let's build some cool visualizations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f9ddc",
   "metadata": {},
   "source": [
    "First, we need to **move the streamlit dashboard script** from the sample scripts folder to the main folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5846b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv \"sample scripts/build_streamlit_dashboard.py\" ~/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d66e0fc",
   "metadata": {},
   "source": [
    "Let's open up the \"build_streamlit_dashboard.py\" file.\n",
    "\n",
    "Since you already had quite a lot of work to do in the data job, we have build the streamlit script entirely for you! The **only required input** from you is in line 39 where you have to enter the name of the weekly correlation table that we populated in script \"40_calculate_correlation.py\". \n",
    "\n",
    "Let's go through the contents of the Streamlit dashboard script and see what is done inside. \n",
    "\n",
    "After the introductory parts (importing libraries and setting up title and description of the dashboard - line 1-22), we **create a connection to the Trino DB** where our data is stored (lines 25-36). Pay attention that since we will run this script outside of VDK, we cannot use it's `execute_query` method and we need to set up a manual connection using the trino Python package. To get the necessary configurations for the connection, we use environment variables (the statements that look like `os.environ.get(variable_name)`) that were initialized in the \"start\" system file (located in the main directory) that MyBinder uses to set up the environment in which we are currently executing our scripts. If we are running those scripts locally on our computers (outside of MyBinder), then the contents of the \"start\" system file should be executed from a terminal before running a data job.\n",
    "\n",
    "In lines 38-40 we read data from the weekly correlation table using pandas `read_sql_query` method that directly converts the output into a pandas data frame. We transform the date into datetime format (line 42) and then **make a line plot using matplotlib Python package**. The plot will show the number of weekly COVID-19 cases versus \"no scent\" complaints over time. \n",
    "\n",
    "After that we build another line plot that will show **how the correlation coefficients change over time**. In line 66 we first show the current correlation coefficient as a KPI and below we show the line plot (lines 68-71). This time it's plotted using streamlit's built-in method `st.line_chart()`. The last thing we display in the dashboard is a table with the weeks and the respective correaltion coefficients (lines 72-78)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bca9f6-8574-4ce5-a89c-9b47c1effd94",
   "metadata": {},
   "source": [
    "**That's it! Let's now run the Streamlit app and see how the dashboard looks like.**\n",
    "\n",
    "Run the two cells below one after the other. The first cell will output a link that you can copy-paste into a browser window ONLY AFTER you run the second cell which is the actual activation of the Streamlit application.\n",
    "\n",
    "If you want to go back to this notebook and make changes to it, you need to kill the Streamlit App by pressing the \"Interrupt the Kernel\" button (the little square button on the tools ribbon at the top)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f543c-a594-49e1-b6c7-0fadead86be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the necessary link - one of the two links links will open up the Streamlit app, the other will show a '403: Forbidden' message\n",
    "import os\n",
    "print(\"Open streamlit (in a new tab) at this link:\")\n",
    "print(\"https://notebooks.gesis.org/binder/jupyter/user/\" + os.environ.get(\"JUPYTERHUB_USER\") + \"/proxy/8501/\")\n",
    "print(\"https://hub.mybinder.turing.ac.uk/user/\" + os.environ.get(\"JUPYTERHUB_USER\") + \"/proxy/8501/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a493dc7d-8510-4e3b-a26f-0cc125d92fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate the Streamlit app. To view the dashboard, open the link produced by the above cell.\n",
    "!streamlit run build_streamlit_dashboard.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2bc966",
   "metadata": {},
   "source": [
    "<font color='red'>**TO DO!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49280288-c045-4dc0-a91d-ef1aba9c52e0",
   "metadata": {},
   "source": [
    "Let's look through the graphs we plotted and verify whether our assumptions regarding the relationship between COVID cases and \"no scent\" complaints are correct. \n",
    "\n",
    "In the first graph, our expectation is that the lines of number of COVID cases and number of \"no scent\" complaints should move almost in sync and when there's a peak in the # COVID cases, there should also be an increase in the \"no scent\" reviews. \n",
    "\n",
    "As for the change in correlation coefficients over time shown in the second part of the dashboard, we expect correlation to be weakening recently since the Omicron variant of COVID does not result in loss of scent.\n",
    "\n",
    "**Are those expectations confirmed by our analysis?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d02cf",
   "metadata": {},
   "source": [
    "<font color='green'>**CONGRATULATIONS!**</font>\n",
    "\n",
    "You reached the end of this example! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
